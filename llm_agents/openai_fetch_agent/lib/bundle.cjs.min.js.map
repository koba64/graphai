{"version":3,"file":"bundle.cjs.min.js","sources":["../../llm_utils/lib/index.js","../src/openai_fetch_agent.ts"],"sourcesContent":["\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getMessages = exports.getMergeValue = exports.flatString = void 0;\nconst flatString = (input) => {\n    return Array.isArray(input) ? input.filter((a) => a).join(\"\\n\") : (input ?? \"\");\n};\nexports.flatString = flatString;\nconst getMergeValue = (namedInputs, params, key, values) => {\n    const inputValue = namedInputs[key];\n    const paramsValue = params[key];\n    return inputValue || paramsValue ? [(0, exports.flatString)(inputValue), (0, exports.flatString)(paramsValue)].filter((a) => a).join(\"\\n\") : (0, exports.flatString)(values);\n};\nexports.getMergeValue = getMergeValue;\nconst getMessages = (systemPrompt, messages) => {\n    const messagesCopy = [...(systemPrompt ? [{ role: \"system\", content: systemPrompt }] : []), ...(messages ?? [])];\n    return messagesCopy;\n};\nexports.getMessages = getMessages;\n","import OpenAI from \"openai\";\nimport { AgentFunction, AgentFunctionInfo, sleep } from \"graphai\";\nimport { GraphAILLMInputBase, getMergeValue, getMessages } from \"@graphai/llm_utils\";\n\ntype OpenAIInputs = {\n  model?: string;\n  images?: string[];\n  tools?: OpenAI.ChatCompletionTool[];\n  tool_choice?: OpenAI.ChatCompletionToolChoiceOption;\n  max_tokens?: number;\n  verbose?: boolean;\n  temperature?: number;\n  messages?: Array<OpenAI.ChatCompletionMessageParam>;\n  response_format?: any;\n} & GraphAILLMInputBase;\n\ntype OpenAIConfig = {\n  baseURL?: string;\n  apiKey?: string;\n  stream?: boolean;\n};\n\ntype OpenAIParams = OpenAIInputs & OpenAIConfig;\n\nconst convertOpenAIChatCompletion = (response: OpenAI.ChatCompletion, messages: OpenAI.ChatCompletionMessageParam[]) => {\n  const message = response?.choices[0] && response?.choices[0].message ? response?.choices[0].message : null;\n  const text = message && message.content ? message.content : null;\n\n  const functionResponse = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0] : null;\n  // const functionId = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0]?.id : null;\n\n  const tool = functionResponse\n    ? {\n        id: functionResponse.id,\n        name: functionResponse?.function?.name,\n        arguments: (() => {\n          try {\n            return JSON.parse(functionResponse?.function?.arguments);\n          } catch (__e) {\n            return undefined;\n          }\n        })(),\n      }\n    : undefined;\n\n  if (message) {\n    messages.push(message);\n  }\n  return {\n    ...response,\n    text,\n    tool,\n    message,\n    messages,\n  };\n};\n\nexport const openAIFetchAgent: AgentFunction<OpenAIParams, Record<string, any> | string, OpenAIInputs, OpenAIConfig> = async ({\n  filterParams,\n  params,\n  namedInputs,\n  config,\n}) => {\n  const { verbose, system, images, temperature, tools, tool_choice, max_tokens, prompt, messages, response_format } = {\n    ...params,\n    ...namedInputs,\n  };\n\n  const { apiKey, stream, baseURL } = {\n    ...params,\n    ...(config || {}),\n  };\n\n  const userPrompt = getMergeValue(namedInputs, params, \"mergeablePrompts\", prompt);\n  const systemPrompt = getMergeValue(namedInputs, params, \"mergeableSystem\", system);\n\n  const messagesCopy = getMessages<OpenAI.ChatCompletionMessageParam>(systemPrompt, messages);\n\n  if (!apiKey) {\n    throw new Error(\"OPENAI_API_KEY key is not set in params. params: {apiKey: 'sk-xxx'}\");\n  }\n\n  if (userPrompt) {\n    messagesCopy.push({\n      role: \"user\",\n      content: userPrompt,\n    });\n  }\n  if (images) {\n    const image_url =\n      params.model === \"gpt-4-vision-preview\"\n        ? images[0]\n        : {\n            url: images[0],\n            detail: \"high\",\n          };\n    messagesCopy.push({\n      role: \"user\",\n      content: [\n        {\n          type: \"image_url\",\n          image_url,\n        } as OpenAI.ChatCompletionContentPart,\n      ],\n    });\n  }\n\n  if (verbose) {\n    console.log(messagesCopy);\n  }\n\n  const chatParams = {\n    model: params.model || \"gpt-4o\",\n    messages: messagesCopy as unknown as OpenAI.ChatCompletionMessageParam[],\n    tools,\n    tool_choice,\n    max_tokens,\n    temperature: temperature ?? 0.7,\n    stream: !!stream,\n    response_format,\n  };\n\n  const urlPrefix = baseURL ?? \"https://api.openai.com/v1\";\n  const response = await fetch(urlPrefix + \"/chat/completions\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      Authorization: `Bearer ${apiKey}`,\n    },\n    body: JSON.stringify(chatParams),\n  });\n\n  if (!stream) {\n    if (response.status === 200) {\n      const result = await response.json();\n      return convertOpenAIChatCompletion(result, messagesCopy);\n    }\n    throw new Error(\"OPENAI API Error\");\n  }\n\n  // streaming\n  const reader = response.body?.getReader();\n\n  if (response.status !== 200 || !reader) {\n    throw new Error(\"Request failed\");\n  }\n\n  const decoder = new TextDecoder(\"utf-8\");\n  let done = false;\n  const buffer = [];\n  let text_buffer = \"\";\n  while (!done) {\n    const { done: readDone, value } = await reader.read();\n    if (readDone) {\n      done = readDone;\n      reader.releaseLock();\n    } else {\n      const text = decoder.decode(value, { stream: true });\n      text_buffer = text + text_buffer;\n      const lines = text_buffer.split(/\\n+/);\n      const next_buff = [];\n      for (const line of lines) {\n        try {\n          const json_text = line.replace(/^data:\\s*/, \"\");\n          if (json_text === \"[DONE]\") {\n            break;\n          } else if (json_text) {\n            const data = JSON.parse(json_text);\n            const token = data.choices[0].delta.content;\n            if (token) {\n              buffer.push(token);\n              if (filterParams && filterParams.streamTokenCallback && token) {\n                filterParams.streamTokenCallback(token);\n              }\n            }\n          }\n        } catch (__error) {\n          next_buff.push(line);\n        }\n      }\n      text_buffer = next_buff.join(\"\\n\");\n    }\n  }\n  return convertOpenAIChatCompletion(\n    {\n      choices: [\n        {\n          message: {\n            role: \"assistant\",\n            content: buffer.join(\"\"),\n            refusal: \"\",\n          },\n        },\n      ],\n    } as any,\n    messagesCopy,\n  );\n};\n\nconst input_sample = \"this is response result\";\nconst result_sample = {\n  object: \"chat.completion\",\n  id: \"chatcmpl-9N7HxXYbwjmdbdiQE94MHoVluQhyt\",\n  choices: [\n    {\n      message: {\n        role: \"assistant\",\n        content: input_sample,\n      },\n      finish_reason: \"stop\",\n      index: 0,\n      logprobs: null,\n    },\n  ],\n  created: 1715296589,\n  model: \"gpt-3.5-turbo-0125\",\n};\n\nexport const openAIMockAgent: AgentFunction<\n  {\n    model?: string;\n    query?: string;\n    system?: string;\n    verbose?: boolean;\n    temperature?: number;\n  },\n  Record<string, any> | string,\n  string | Array<any>\n> = async ({ filterParams }) => {\n  for await (const token of input_sample.split(\"\")) {\n    if (filterParams && filterParams.streamTokenCallback && token) {\n      await sleep(100);\n      filterParams.streamTokenCallback(token);\n    }\n  }\n\n  return result_sample;\n};\nconst openAIFetchAgentInfo: AgentFunctionInfo = {\n  name: \"openAIFetchAgent\",\n  agent: openAIFetchAgent,\n  mock: openAIMockAgent,\n  inputs: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: {\n        anyOf: [{ type: \"array\" }, { type: \"object\" }],\n      },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      baseURL: { type: \"string\" },\n      apiKey: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }],\n      },\n      stream: { type: \"boolean\" },\n      prompt: {\n        type: \"string\",\n        description: \"query string\",\n      },\n      messages: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }],\n        description: \"chat messages\",\n      },\n    },\n  },\n  output: {\n    type: \"object\",\n    properties: {\n      id: {\n        type: \"string\",\n      },\n      object: {\n        type: \"string\",\n      },\n      created: {\n        type: \"integer\",\n      },\n      model: {\n        type: \"string\",\n      },\n      choices: {\n        type: \"array\",\n        items: [\n          {\n            type: \"object\",\n            properties: {\n              index: {\n                type: \"integer\",\n              },\n              message: {\n                type: \"array\",\n                items: [\n                  {\n                    type: \"object\",\n                    properties: {\n                      content: {\n                        type: \"string\",\n                      },\n                      role: {\n                        type: \"string\",\n                      },\n                    },\n                    required: [\"content\", \"role\"],\n                  },\n                ],\n              },\n            },\n            required: [\"index\", \"message\", \"logprobs\", \"finish_reason\"],\n          },\n        ],\n      },\n      usage: {\n        type: \"object\",\n        properties: {\n          prompt_tokens: {\n            type: \"integer\",\n          },\n          completion_tokens: {\n            type: \"integer\",\n          },\n          total_tokens: {\n            type: \"integer\",\n          },\n        },\n        required: [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"],\n      },\n      text: {\n        type: \"string\",\n      },\n      tool: {\n        arguments: {\n          type: \"object\",\n        },\n        name: {\n          type: \"string\",\n        },\n      },\n      message: {\n        type: \"object\",\n        properties: {\n          content: {\n            type: \"string\",\n          },\n          role: {\n            type: \"string\",\n          },\n        },\n        required: [\"content\", \"role\"],\n      },\n    },\n    required: [\"id\", \"object\", \"created\", \"model\", \"choices\", \"usage\"],\n  },\n  params: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: { anyOf: [{ type: \"array\" }, { type: \"object\" }] },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      baseURL: { type: \"string\" },\n      apiKey: { anyOf: [{ type: \"string\" }, { type: \"object\" }] },\n      stream: { type: \"boolean\" },\n      prompt: { type: \"string\", description: \"query string\" },\n      messages: { anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }], description: \"chat messages\" },\n    },\n  },\n  outputFormat: {\n    llmResponse: {\n      key: \"choices.$0.message.content\",\n      type: \"string\",\n    },\n  },\n  samples: [\n    {\n      inputs: { prompt: input_sample },\n      params: {},\n      result: result_sample,\n    },\n  ],\n  description: \"OpenAI Fetch Agent\",\n  category: [\"llm\"],\n  author: \"Receptron team\",\n  repository: \"https://github.com/receptron/graphai\",\n  license: \"MIT\",\n  stream: true,\n  npms: [\"openai\"],\n};\n\nexport default openAIFetchAgentInfo;\n"],"names":["Object","defineProperty","exports","value","getMessages","getMergeValue","flatString","input","Array","isArray","filter","a","join","namedInputs","params","key","values","inputValue","paramsValue","systemPrompt","messages","role","content","convertOpenAIChatCompletion","response","message","choices","text","functionResponse","tool_calls","tool","id","name","function","arguments","JSON","parse","__e","undefined","push","input_sample","result_sample","object","finish_reason","index","logprobs","created","model","openAIFetchAgentInfo","agent","async","filterParams","config","verbose","system","images","temperature","tools","tool_choice","max_tokens","prompt","response_format","apiKey","stream","baseURL","userPrompt","messagesCopy","Error","image_url","url","detail","type","console","log","chatParams","urlPrefix","fetch","method","headers","Authorization","body","stringify","status","result","json","reader","getReader","decoder","TextDecoder","done","buffer","text_buffer","readDone","read","releaseLock","decode","lines","split","next_buff","line","json_text","replace","token","delta","streamTokenCallback","__error","refusal","mock","sleep","inputs","properties","anyOf","description","output","items","required","usage","prompt_tokens","completion_tokens","total_tokens","outputFormat","llmResponse","samples","category","author","repository","license","npms"],"mappings":"wEACAA,OAAOC,eAAcC,EAAU,aAAc,CAAEC,OAAO,IACtDD,EAAAE,YAAsBF,EAAwBG,cAAAH,EAAAI,gBAAqB,EAInEJ,EAAAI,WAHoBC,GACTC,MAAMC,QAAQF,GAASA,EAAMG,QAAQC,GAAMA,IAAGC,KAAK,MAASL,GAAS,GAQhFL,EAAAG,cALsB,CAACQ,EAAaC,EAAQC,EAAKC,KAC7C,MAAMC,EAAaJ,EAAYE,GACzBG,EAAcJ,EAAOC,GAC3B,OAAOE,GAAcC,EAAc,EAAC,EAAIhB,EAAQI,YAAYW,IAAa,EAAIf,EAAQI,YAAYY,IAAcR,QAAQC,GAAMA,IAAGC,KAAK,OAAQ,EAAIV,EAAQI,YAAYU,EAAO,EAOhLd,EAAAE,YAJoB,CAACe,EAAcC,IACV,IAAKD,EAAe,CAAC,CAAEE,KAAM,SAAUC,QAASH,IAAkB,MAASC,GAAY,YCUhH,MAAMG,EAA8B,CAACC,EAAiCJ,KACpE,MAAMK,EAAUD,GAAUE,QAAQ,IAAMF,GAAUE,QAAQ,GAAGD,QAAUD,GAAUE,QAAQ,GAAGD,QAAU,KAChGE,EAAOF,GAAWA,EAAQH,QAAUG,EAAQH,QAAU,KAEtDM,EAAmBH,GAASI,YAAcJ,GAASI,WAAW,GAAKJ,GAASI,WAAW,GAAK,KAG5FC,EAAOF,EACT,CACEG,GAAIH,EAAiBG,GACrBC,KAAMJ,GAAkBK,UAAUD,KAClCE,UAAW,MACT,IACE,OAAOC,KAAKC,MAAMR,GAAkBK,UAAUC,WAC9C,MAAOG,GACP,OAEH,EANU,SAQbC,EAKJ,OAHIb,GACFL,EAASmB,KAAKd,GAET,IACFD,EACHG,OACAG,OACAL,UACAL,WACD,EAiJGoB,EAAe,0BACfC,EAAgB,CACpBC,OAAQ,kBACRX,GAAI,yCACJL,QAAS,CACP,CACED,QAAS,CACPJ,KAAM,YACNC,QAASkB,GAEXG,cAAe,OACfC,MAAO,EACPC,SAAU,OAGdC,QAAS,WACTC,MAAO,sBAuBHC,EAA0C,CAC9ChB,KAAM,mBACNiB,MAvLqHC,OACrHC,eACArC,SACAD,cACAuC,aAEA,MAAMC,QAAEA,EAAOC,OAAEA,EAAMC,OAAEA,EAAMC,YAAEA,EAAWC,MAAEA,EAAKC,YAAEA,EAAWC,WAAEA,EAAUC,OAAEA,EAAMxC,SAAEA,EAAQyC,gBAAEA,GAAoB,IAC/G/C,KACAD,IAGCiD,OAAEA,EAAMC,OAAEA,EAAMC,QAAEA,GAAY,IAC/BlD,KACCsC,GAAU,CAAA,GAGVa,EAAa5D,EAAAA,cAAcQ,EAAaC,EAAQ,mBAAoB8C,GACpEzC,EAAed,EAAAA,cAAcQ,EAAaC,EAAQ,kBAAmBwC,GAErEY,EAAe9D,EAAAA,YAA+Ce,EAAcC,GAElF,IAAK0C,EACH,MAAM,IAAIK,MAAM,uEASlB,GANIF,GACFC,EAAa3B,KAAK,CAChBlB,KAAM,OACNC,QAAS2C,IAGTV,EAAQ,CACV,MAAMa,EACa,yBAAjBtD,EAAOiC,MACHQ,EAAO,GACP,CACEc,IAAKd,EAAO,GACZe,OAAQ,QAEhBJ,EAAa3B,KAAK,CAChBlB,KAAM,OACNC,QAAS,CACP,CACEiD,KAAM,YACNH,gBAMJf,GACFmB,QAAQC,IAAIP,GAGd,MAAMQ,EAAa,CACjB3B,MAAOjC,EAAOiC,OAAS,SACvB3B,SAAU8C,EACVT,QACAC,cACAC,aACAH,YAAaA,GAAe,GAC5BO,SAAUA,EACVF,mBAGIc,EAAYX,GAAW,4BACvBxC,QAAiBoD,MAAMD,EAAY,oBAAqB,CAC5DE,OAAQ,OACRC,QAAS,CACP,eAAgB,mBAChBC,cAAe,UAAUjB,KAE3BkB,KAAM7C,KAAK8C,UAAUP,KAGvB,IAAKX,EAAQ,CACX,GAAwB,MAApBvC,EAAS0D,OAAgB,CAC3B,MAAMC,QAAe3D,EAAS4D,OAC9B,OAAO7D,EAA4B4D,EAAQjB,GAE7C,MAAM,IAAIC,MAAM,oBAIlB,MAAMkB,EAAS7D,EAASwD,MAAMM,YAE9B,GAAwB,MAApB9D,EAAS0D,SAAmBG,EAC9B,MAAM,IAAIlB,MAAM,kBAGlB,MAAMoB,EAAU,IAAIC,YAAY,SAChC,IAAIC,GAAO,EACX,MAAMC,EAAS,GACf,IAAIC,EAAc,GAClB,MAAQF,GAAM,CACZ,MAAQA,KAAMG,EAAQzF,MAAEA,SAAgBkF,EAAOQ,OAC/C,GAAID,EACFH,EAAOG,EACPP,EAAOS,kBACF,CAELH,EADaJ,EAAQQ,OAAO5F,EAAO,CAAE4D,QAAQ,IACxB4B,EACrB,MAAMK,EAAQL,EAAYM,MAAM,OAC1BC,EAAY,GAClB,IAAK,MAAMC,KAAQH,EACjB,IACE,MAAMI,EAAYD,EAAKE,QAAQ,YAAa,IAC5C,GAAkB,WAAdD,EACF,MACK,GAAIA,EAAW,CACpB,MACME,EADOnE,KAAKC,MAAMgE,GACL1E,QAAQ,GAAG6E,MAAMjF,QAChCgF,IACFZ,EAAOnD,KAAK+D,GACRnD,GAAgBA,EAAaqD,qBAAuBF,GACtDnD,EAAaqD,oBAAoBF,KAIvC,MAAOG,GACPP,EAAU3D,KAAK4D,GAGnBR,EAAcO,EAAUtF,KAAK,OAGjC,OAAOW,EACL,CACEG,QAAS,CACP,CACED,QAAS,CACPJ,KAAM,YACNC,QAASoE,EAAO9E,KAAK,IACrB8F,QAAS,OAKjBxC,EACD,EA6CDyC,KAbEzD,OAASC,mBACX,UAAW,MAAMmD,KAAS9D,EAAayD,MAAM,IACvC9C,GAAgBA,EAAaqD,qBAAuBF,UAChDM,EAAAA,MAAM,KACZzD,EAAaqD,oBAAoBF,IAIrC,OAAO7D,CAAa,EAMpBoE,OAAQ,CACNtC,KAAM,SACNuC,WAAY,CACV/D,MAAO,CAAEwB,KAAM,UACfjB,OAAQ,CAAEiB,KAAM,UAChBd,MAAO,CAAEc,KAAM,UACfb,YAAa,CACXqD,MAAO,CAAC,CAAExC,KAAM,SAAW,CAAEA,KAAM,YAErCZ,WAAY,CAAEY,KAAM,UACpBlB,QAAS,CAAEkB,KAAM,WACjBf,YAAa,CAAEe,KAAM,UACrBP,QAAS,CAAEO,KAAM,UACjBT,OAAQ,CACNiD,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,YAEtCR,OAAQ,CAAEQ,KAAM,WAChBX,OAAQ,CACNW,KAAM,SACNyC,YAAa,gBAEf5F,SAAU,CACR2F,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UACxDyC,YAAa,mBAInBC,OAAQ,CACN1C,KAAM,SACNuC,WAAY,CACV/E,GAAI,CACFwC,KAAM,UAER7B,OAAQ,CACN6B,KAAM,UAERzB,QAAS,CACPyB,KAAM,WAERxB,MAAO,CACLwB,KAAM,UAER7C,QAAS,CACP6C,KAAM,QACN2C,MAAO,CACL,CACE3C,KAAM,SACNuC,WAAY,CACVlE,MAAO,CACL2B,KAAM,WAER9C,QAAS,CACP8C,KAAM,QACN2C,MAAO,CACL,CACE3C,KAAM,SACNuC,WAAY,CACVxF,QAAS,CACPiD,KAAM,UAERlD,KAAM,CACJkD,KAAM,WAGV4C,SAAU,CAAC,UAAW,YAK9BA,SAAU,CAAC,QAAS,UAAW,WAAY,oBAIjDC,MAAO,CACL7C,KAAM,SACNuC,WAAY,CACVO,cAAe,CACb9C,KAAM,WAER+C,kBAAmB,CACjB/C,KAAM,WAERgD,aAAc,CACZhD,KAAM,YAGV4C,SAAU,CAAC,gBAAiB,oBAAqB,iBAEnDxF,KAAM,CACJ4C,KAAM,UAERzC,KAAM,CACJI,UAAW,CACTqC,KAAM,UAERvC,KAAM,CACJuC,KAAM,WAGV9C,QAAS,CACP8C,KAAM,SACNuC,WAAY,CACVxF,QAAS,CACPiD,KAAM,UAERlD,KAAM,CACJkD,KAAM,WAGV4C,SAAU,CAAC,UAAW,UAG1BA,SAAU,CAAC,KAAM,SAAU,UAAW,QAAS,UAAW,UAE5DrG,OAAQ,CACNyD,KAAM,SACNuC,WAAY,CACV/D,MAAO,CAAEwB,KAAM,UACfjB,OAAQ,CAAEiB,KAAM,UAChBd,MAAO,CAAEc,KAAM,UACfb,YAAa,CAAEqD,MAAO,CAAC,CAAExC,KAAM,SAAW,CAAEA,KAAM,YAClDZ,WAAY,CAAEY,KAAM,UACpBlB,QAAS,CAAEkB,KAAM,WACjBf,YAAa,CAAEe,KAAM,UACrBP,QAAS,CAAEO,KAAM,UACjBT,OAAQ,CAAEiD,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,YAC9CR,OAAQ,CAAEQ,KAAM,WAChBX,OAAQ,CAAEW,KAAM,SAAUyC,YAAa,gBACvC5F,SAAU,CAAE2F,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UAAYyC,YAAa,mBAGjGQ,aAAc,CACZC,YAAa,CACX1G,IAAK,6BACLwD,KAAM,WAGVmD,QAAS,CACP,CACEb,OAAQ,CAAEjD,OAAQpB,GAClB1B,OAAQ,CAAE,EACVqE,OAAQ1C,IAGZuE,YAAa,qBACbW,SAAU,CAAC,OACXC,OAAQ,iBACRC,WAAY,uCACZC,QAAS,MACT/D,QAAQ,EACRgE,KAAM,CAAC"}